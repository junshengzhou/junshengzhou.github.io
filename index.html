<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- NOTE: Do not scrape the code from this page directly, as it includes analytics tags. You are welcome to use the HTML in this page, but please do so by cloning the github repo linked at the bottom. -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QSFRPR5L7E"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-QSFRPR5L7E');
  </script>

  <title>Junsheng Zhou | å‘¨ä¿Šæ˜‡</title>
  
  <meta name="author" content="Junsheng Zhou | å‘¨ä¿Šæ˜‡">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸŒ </text></svg>">
  <script src="script/functions.js"></script>
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:62%;vertical-align:middle">
              <p style="text-align:center">
                <name>Junsheng Zhou | å‘¨ä¿Šæ˜‡</name>
              </p>
              <p>I am currently a Ph.D. student in <a href="https://www.thss.tsinghua.edu.cn/">School of Software</a>, <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>, advised by Prof. <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>.
              </p>
              <p>
                My research interests lie in the area of 3D computer vision and grapics, especially in 3D reconstruction, generative models, 3D foundation models and cross-modal learning. 
              </p>
              <p style="text-align:center">
                <a href="mailto:zhoujs21@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=afPIrLYAAAAJ&hl=zh-CN&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/junshengzhou/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="media/profile.png"><img style="width:90%;max-width:100%" alt="profile photo" src="media/profile.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>News</heading>
                <ul>
                  <li><strong>06/2024:</strong> Our paper <a href="https://mabaorui.github.io/Noise2NoiseMapping/">FastN2N</a> on fast learning of implicit 3D representations is accepted to <b>TPAMI 2024</b>.
                  <li><strong>04/2024:</strong> The extension of <a href="https://junshengzhou.github.io/CAP-UDF/">CAP-UDF</a> on implicit representations is accepted to <b>TPAMI 2024</b>.
                  <li><strong>03/2024:</strong> Our paper <a href="https://weiqi-zhang.github.io/UDiFF/">UDiFF</a> on UDF-based 3D generative models is accepted to <b>CVPR 2024</b>.
                  <li><strong>03/2024:</strong> I will co-organize workshop <a href="https://emai-workshop.github.io/">EMbodied AI: Trends, Challenges, and Opportunities</a> in <b>ICIP 2024</b>.

                  <li><strong>02/2024:</strong> Our paper <a href="https://github.io">3D-OAE</a> on 3D foundation models is accepted to <b>ICRA 2024</b> for <strong><span style="color:#ff0000;">Oral</span></strong> presentation.

                  <li><strong>01/2024:</strong> Our work <a href="https://github.com/baaivision/Uni3D">Uni3D</a> on scaling up 3D foundation models is accepted to <b>ICLR 2024</b> <strong><span style="color:#ff0000;">(Spotlight)</span></strong>.
                    <!-- <li><strong>01/2024:</strong> Our work <a href="">Uni3D</a> is accepted to <b>ICLR 2024</b> <strong><span style="color:#ff0000;">(Spotlight)</span></strong>. -->
                  <li><strong>12/2023:</strong> Two papers on multi-view reconstruction and point upsampling is accepted to <b>AAAI 2024</b>.
                  <li><strong>10/2023:</strong> Releasing <a href="https://github.com/baaivision/Uni3D">Uni3D</a>, a unified 3D foundation model with <strong><span style="color:#ff0000;">one billion</span></strong> parameters.
                  </li>
                  <li><strong>09/2023:</strong> Our paper <a href="">VP2P-Match</a> on image to LiDAR point cloud registration is accepted to <b>NeurIPS 2023</b> <strong><span style="color:#ff0000;">(Spotlight)</span></strong>.
                  </li>
                  <li><strong>08/2023:</strong> Our paper <a href="https://github.com/junshengzhou/LevelSetUDF/">LevelSetUDF</a> on neural implicit representations is accepted to <b>ICCV 2023</b>.
                  </li>
                  <li><strong>06/2023:</strong> One paper on surface reconstruction that I advised is accepted to <b>SMI 2023</b> and <b>C&G 2023</b>.
                  </li>
                  <li><strong>03/2023:</strong> Our paper <a href="https://github.com/mabaorui/TowardsBetterGradient/">LSA-SDF</a> on implicit representation is accepted to <b>CVPR 2023</b>.
                  </li>
                  <li><strong>02/2023:</strong> Invited to give a talk about implicit surface reconstruction (<a href="https://junshengzhou.github.io/CAP-UDF/">CAP-UDF</a>) at <b>AI TIME</b> (<a href="https://www.bilibili.com/video/BV1vX4y197G6/?spm_id_from=333.999.0.0&vd_source=12a96146b06f9a1687080af1ce34dcf1">Video</a>).
                  </li>

                  <li><strong>11/2022:</strong> Our paper <a href="https://github.com/lisj575/NeAF/">NeAF</a> on implicit point normal estimation is accepted to <b>AAAI 2023</b> <strong><span style="color:#ff0000;">(Oral)</span></strong>.
                  </li>

                  <a href="javascript:toggleblock(&#39;old_news&#39;)">---- show more ----</a>
                  <div id="old_news" style="display: none;">

                  <li><strong>09/2022:</strong> Our paper <a href="https://junshengzhou.github.io/CAP-UDF/">CAP-UDF</a> on surface reconstruction is accepted to <b>NeurIPS 2022</b>.
                  </li>
                  <li><strong>03/2022:</strong> Our paper <a href='https://github.com/junshengzhou/3DAttriFlow'>3DAttriFlow</a> on point cloud generation is accepted to <b>CVPR 2022.</b>
                  </li>
                  <li><strong>11/2021:</strong> My team won the <strong><span style="color:#ff0000;">3rd Place</span></strong> in the <a href='https://mvp-dataset.github.io/'>MVP Completion Challenge</a> (<b>ICCV 2021</b> Workshop).
                  </li>
                  <li><strong>10/2021:</strong> Our paper MSM-Vis on data visualization won the <strong><span style="color:#ff0000;">Best Poster Paper</span></strong> on <b>ChinaVR 2021</b>.
                  </li>
                  <li><strong>09/2021:</strong> Started master journey at <a href="https://www.tsinghua.edu.cn/">Tsinghua University</a>.
                  </li>

                  </div>

                </ul>
    
              </td>
            </tr>
            </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>(<strong>*</strong> Equal Contribution)</p>
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="fastn2n_stop()" onmouseover="fastn2n_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="fastn2n_shape" style="opacity: 0;">
                  <img src='media/FastN2N_after.gif' width="225">
                </div>
                <img src="media/FastN2N_before.png" width="225">
        
              </div>
              <script type="text/javascript">
                function fastn2n_start() {
                  document.getElementById('fastn2n_shape').style.opacity = "1";
                }
                function fastn2n_stop() {
                  document.getElementById('fastn2n_shape').style.opacity = "0";
                }
                fastn2n_stop()
              </script>
            </td>
        
            <td width="65%" valign="top">
              <a href="https://mabaorui.github.io/Noise2NoiseMapping/">
                <papertitle>Fast Learning of Signed Distance Functions from Noisy Point Clouds via Noise to Noise Mapping</papertitle>
              </a>
              <br>
              <strong>Junsheng Zhou*</strong>, <a href="https://mabaorui.github.io/"> Baorui Ma*</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b></em>, 2024
              <br>
              <a href="https://mabaorui.github.io/Noise2NoiseMapping/">project page</a> |             
              <a href="https://ieeexplore.ieee.org/document/10561582">IEEE Xplore</a> | <a href="https://arxiv.org/abs/2407.14225">arXiv</a> |
              <a href="https://github.com/mabaorui/Noise2NoiseMapping">code</a>
        
              <p align="justify", style="font-size:13px">We present a fast learning framework capable of inferring signed distance functions from noisy shapes within one minute through noise-to-noise mapping.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="UDiFF_stop()" onmouseover="UDiFF_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="UDiFF" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/UDiFF_after_2.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="media/UDiFF_before.png" width="225">
        
              </div>
        
              <script type="text/javascript">
                function UDiFF_start() {
                  document.getElementById('UDiFF').style.opacity = "1";
                }
                function UDiFF_stop() {
                  document.getElementById('UDiFF').style.opacity = "0";
                }
                UDiFF_stop()
              </script>
            </td>
        
            <td width="65%" valign="top">
              <a href="https://weiqi-zhang.github.io/UDiFF/">
                <papertitle>UDiFF: Generating Conditional Unsigned Distance Fields with Optimal Wavelet Diffusion
                </papertitle>
              </a>
              <br>
              <strong>Junsheng Zhou*</strong>, Weiqi Zhang*, <a href="https://mabaorui.github.io/"> Baorui Ma</a>, Kanle Shi, 
              <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2024
              <br>
              <a href="https://weiqi-zhang.github.io/UDiFF/">project page</a> |             
              <a href="https://arxiv.org/abs/2404.06851">arXiv</a> |
              <a href="https://github.com/weiqi-zhang/UDiFF">code</a>
        
              <p align="justify", style="font-size:13px">UDiFF is a 3D diffusion model for unsigned distance fields (UDFs) which is capable to generate textured 3D shapes with open surfaces from text conditions or unconditionally.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="cappami_stop()" onmouseover="cappami_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="cappami_shape" style="opacity: 0;">
                  <img src='media/cappami_before.png' width="225">
                </div>
                <img src="media/cappami_after.png" width="225">
        
              </div>
              <script type="text/javascript">
                function cappami_start() {
                  document.getElementById('cappami_shape').style.opacity = "1";
                }
                function cappami_stop() {
                  document.getElementById('cappami_shape').style.opacity = "0";
                }
                cappami_stop()
              </script>
            </td>
        
            <td width="65%" valign="top">
              <a href="https://junshengzhou.github.io/CAP-UDF/">
                <papertitle>CAP-UDF: Learning Unsigned Distance Functions Progressively from Raw Point Clouds with Consistency-Aware Field Optimization</papertitle>
              </a>
              <br>
              <strong>Junsheng Zhou*</strong>, <a href="https://mabaorui.github.io/"> Baorui Ma*</a>, Shujuan Li, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://mmvc.engineering.nyu.edu/">Yi Fang</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence <b>(TPAMI)</b></em>, 2024
              <br>
              <a href="https://junshengzhou.github.io/CAP-UDF/">project page</a> |             
              <a href="https://ieeexplore.ieee.org/document/10506631">IEEE Xplore</a> | <a href="https://arxiv.org/pdf/2210.02757.pdf">arXiv</a> |
              <a href="https://github.com/junshengzhou/CAP-UDF">code</a>
        
              <p align="justify", style="font-size:13px">We present CAP-UDF to represent shapes and scenes with arbitrary architecture by learning a Consistency-Aware unsigned distance function Progressively.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="uni3d_stop()" onmouseover="uni3d_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="uni3d" style="opacity: 0;">
                  <img src='media/uni3d_before.jpg' width="225">
                </div>
                <img src="media/uni3d_after.jpg" width="225">
      
              </div>
              <script type="text/javascript">
                function uni3d_start() {
                  document.getElementById('uni3d').style.opacity = "1";
                }
                function uni3d_stop() {
                  document.getElementById('uni3d').style.opacity = "0";
                }
                uni3d_stop()
              </script>
            </td>
      
            <td width="65%" valign="top">
              <a href="https://github.com/baaivision/Uni3D">
                <papertitle>Uni3D: Exploring Unified 3D Representation at Scale
                </papertitle>
              </a>
              <br>
              <strong>Junsheng Zhou*</strong>, <a href="https://github.com/Wolfwjs/"> Jinsheng Wang*</a>, <a href="https://mabaorui.github.io/"> Baorui Ma*</a>, 
              <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://scholar.google.com/citations?user=knvEK4AAAAAJ&hl=en"> Tiejun Huang</a>, <a href="https://www.xloong.wang/">Xinlong Wang</a>
              <br>
              <em>International Conference on Learning Representations <b>(ICLR)</b></em>, 2024  (<strong><span style="color:#ff0000;">Spotlight</span></strong>)
              <br>
              <a href="https://huggingface.co/BAAI/Uni3D/tree/main/modelzoo">Model Zoo</a> |             
              <a href="https://arxiv.org/abs/2310.06773/">arXiv</a> |
              <a href="https://github.com/baaivision/Uni3D">code</a>
      
              <p align="justify", style="font-size:13px">We present Uni3D, a unified and scalable 3D pretraining framework for large-scale 3D representation learning, and explore its limits at the scale of one billion parameters.
              </p>
            </td>
          </tr>
          </tbody>
        </table>


                  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="oae_stop()" onmouseover="oae_start()">
            <td width="28%">
              <div class="one">
                  <div class="two" id="oae_shape" style="opacity: 0;">
                      <video width="225" muted="" autoplay="" loop="">
                        <source src="media/3doae_after.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                  </div>
                <img src="media/3doae_before.png" width="225">
                <td width="65%" valign="top">
                  <a href="https://junshengzhou.github.io/3D-OAE/">
                    <papertitle>3D-OAE: Occlusion Auto-Encoders for Self-Supervised Learning on Point Clouds</papertitle>
                  </a>
                  <br>
                  <strong>Junsheng Zhou*</strong>, <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN&oi=ao"> Xin Wen*</a>, <a href="https://mabaorui.github.io/"> Baorui Ma</a>,
                  <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://www.gaoyue.org/"> Yue Gao</a>, <a href="http://mmvc.engineering.nyu.edu/">Yi Fang</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
                  <br>
                  <em>IEEE International Conference on Robotics and Automation <b>(ICRA)</b></em>, 2024  (<strong><span style="color:#ff0000;">Oral</span></strong>)
                  <br>
                  <a href="https://junshengzhou.github.io/3D-OAE/">project page</a> |             
                  <a href="https://arxiv.org/pdf/2203.14084.pdf">arXiv</a> |
                  <a href="https://github.com/junshengzhou/3D-OAE">code</a>
        
                  <p align="justify", style="font-size:13px">We present 3D-OAE, a novel self-supervised point cloud representation learning framework which is highly efficient and can be further transferred to various downstream tasks.
                  </p>
                </td>
              </div>
              <script type="text/javascript">
                function oae_start() {
                  document.getElementById('oae_shape').style.opacity = "1";
                }
                function oae_stop() {
                  document.getElementById('oae_shape').style.opacity = "0";
                }
                oae_stop()
              </script>
            </td>


          </tr>
          </tbody>
        </table>



        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="GeoDream_stop()" onmouseover="GeoDream_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="GeoDream" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/GeoDream_after.mov" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="media/GeoDream_before.png" width="225", height="120">
      
              </div>
      
              <script type="text/javascript">
                function GeoDream_start() {
                  document.getElementById('GeoDream').style.opacity = "1";
                }
                function GeoDream_stop() {
                  document.getElementById('GeoDream').style.opacity = "0";
                }
                GeoDream_stop()
              </script>
            </td>
      
            <td width="65%" valign="top">
              <a href="https://mabaorui.github.io/GeoDream_page/">
                <papertitle>GeoDream: Disentangling 2D and Geometric Priors for High-Fidelity and Consistent 3D Generation
                </papertitle>
              </a>
              <br>
              <a href="https://mabaorui.github.io/"> Baorui Ma*</a>, <a href="https://github.com/Bitterdhg/"> Haoge Deng*</a>, <strong>Junsheng Zhou</strong>, 
              <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://scholar.google.com/citations?user=knvEK4AAAAAJ&hl=en"> Tiejun Huang</a>, <a href="https://www.xloong.wang/">Xinlong Wang</a>
              <br>
              <em>arXiv</b></em>, 2024
              <br>
              <a href="https://mabaorui.github.io/GeoDream_page/">project page</a> |             
              <a href="https://arxiv.org/abs/2311.17971/">arXiv</a> |
              <a href="https://github.com/baaivision/GeoDream">code</a>
      
              <p align="justify", style="font-size:13px">We present a 3D generation method that incorporates explicit generalized 3D priors with 2D diffusion priors to enhance the capability of generating unambiguous 3D consistent geometries.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="LDI_stop()" onmouseover="LDI_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="LDI" style="opacity: 0;">
                  <img src='media/LDI_after.gif' width="225", height="120">
                </div>
                <img src="media/LDI_before.png" width="225", height="120">
      
              </div>
      
              <script type="text/javascript">
                function LDI_start() {
                  document.getElementById('LDI').style.opacity = "1";
                }
                function LDI_stop() {
                  document.getElementById('LDI').style.opacity = "0";
                }
                LDI_stop()
              </script>
            </td>
      
            <td width="65%" valign="top">
              <a href="https://lisj575.github.io/APU-LDI/">
                <papertitle>Learning Continuous Implicit Field with Local Distance Indicator for Arbitrary-Scale Point Cloud Upsampling
                </papertitle>
              </a>
              <br>
              Shujuan Li*, <strong>Junsheng Zhou*</strong>, <a href="https://mabaorui.github.io/"> Baorui Ma*</a>, 
              <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2024
              <br>
              <a href="https://lisj575.github.io/APU-LDI/">project page</a> |             
              <a href="https://arxiv.org/abs/2312.15133">arXiv</a> |
              <a href="https://arxiv.org/abs/2312.15133">code</a>
      
              <p align="justify", style="font-size:13px">We propose to learn a Local Distance Indicator as local priors to guide  arbitrary-scale point cloud upsampling.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="NeuSurf_stop()" onmouseover="NeuSurf_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="NeuSurf" style="opacity: 0;">
                  <img src='media/NeuSurf_after.png' width="225", height="120">
                </div>
                <img src="media/NeuSurf_before.png" width="225", height="120">
      
              </div>
      
              <script type="text/javascript">
                function NeuSurf_start() {
                  document.getElementById('NeuSurf').style.opacity = "1";
                }
                function NeuSurf_stop() {
                  document.getElementById('NeuSurf').style.opacity = "0";
                }
                NeuSurf_stop()
              </script>
            </td>
      
            <td width="65%" valign="top">
              <a href="https://alvin528.github.io/NeuSurf/">
                <papertitle>NeuSurf: On-Surface Priors for Neural Surface Reconstruction from Sparse Input Views
                </papertitle>
              </a>
              <br>
              Han Huang, Yulun Wu, <strong>Junsheng Zhou</strong>, Ge Gao, Ming Gu,
              <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>
              <br>
              <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2024
              <br>
              <a href="https://alvin528.github.io/NeuSurf/">project page</a> |             
              <a href="https://arxiv.org/pdf/2312.13977.pdf">arXiv</a> |
              <a href="https://arxiv.org/pdf/2312.13977.pdf">code</a>
      
              <p align="justify", style="font-size:13px">We design a sparse view reconstruction framework that leverages on-surface priors from explicit points to achieve highly faithful surface reconstruction.
              </p>
            </td>
          </tr>
          </tbody>
        </table>
	      
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="VP2P_stop()" onmouseover="VP2P_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="VP2P" style="opacity: 0;">
                  <video width="225" muted="" autoplay="" loop="">
                    <source src="media/Video_VP2PMatching_before.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                  </video>
                </div>
                <img src="media/Video_VP2PMatching_after.png" width="225">
      
              </div>

              <script type="text/javascript">
                function VP2P_start() {
                  document.getElementById('VP2P').style.opacity = "1";
                }
                function VP2P_stop() {
                  document.getElementById('VP2P').style.opacity = "0";
                }
                VP2P_stop()
              </script>
            </td>
      
            <td width="65%" valign="top">
              <a href="https://github.com/junshengzhou/VP2P-Match">
                <papertitle>Differentiable Registration of Images and LiDAR Point Clouds with VoxelPoint-to-Pixel Matching
                </papertitle>
              </a>
              <br>
              <strong>Junsheng Zhou*</strong>, <a href="https://mabaorui.github.io/"> Baorui Ma*</a>, Wenyuan Zhang, <a href="http://mmvc.engineering.nyu.edu/">Yi Fang</a>,
              <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>Conference on Neural Information Processing Systems <b>(NeurIPS)</b></em>, 2023  (<strong><span style="color:#ff0000;">Spotlight</span></strong>)
              <br>
              <a href="https://github.com/junshengzhou/VP2P-Match">project page</a> |             
              <a href="https://arxiv.org/abs/2312.04060">arXiv</a> |
              <a href="https://github.com/junshengzhou/VP2P-Match">code</a>
      
              <p align="justify", style="font-size:13px">We design a triplet network to learn VoxelPoint-to-Pixel matching via a differentiable probabilistic PnP solver.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="ICCVUDF_stop()" onmouseover="ICCVUDF_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="ICCVUDF" style="opacity: 0;">
                  <img src='media/iccv23udf_before.gif' width="225">
                </div>
                <img src="media/ICCVUDF_after.png" width="225">

              </div>
              <script type="text/javascript">
                function ICCVUDF_start() {
                  document.getElementById('ICCVUDF').style.opacity = "1";
                }
                function ICCVUDF_stop() {
                  document.getElementById('ICCVUDF').style.opacity = "0";
                }
                ICCVUDF_stop()
              </script>
            </td>

            <td width="65%" valign="top">
              <a href="https://github.com/junshengzhou/LevelSetUDF">
                <papertitle>Learning a More Continuous Zero Level Set in Unsigned Distance Fields through Level Set Projection
                </papertitle>
              </a>
              <br>
              <strong>Junsheng Zhou*</strong>, <a href="https://mabaorui.github.io/"> Baorui Ma*</a>, Shujuan Li, 
              <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>IEEE/CVF International Conference on Computer Vision <b>(ICCV)</b></em>, 2023
              <br>
              <a href="https://github.com/junshengzhou/LevelSetUDF">project page</a> |             
              <a href="https://arxiv.org/abs/2308.11441/">arXiv</a> |
              <a href="https://github.com/junshengzhou/LevelSetUDF">code</a>

              <p align="justify", style="font-size:13px">We propose to guide the learning of zero level set in UDF using the rest non-zero level sets via a projection procedure.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="LSA_stop()" onmouseover="LSA_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="LSA" style="opacity: 0;">
                  <img src='media/GradientConsistency_before.gif' width="225">
                </div>
                <img src="media/LSA_after.png" width="225">

              </div>
              <script type="text/javascript">
                function LSA_start() {
                  document.getElementById('LSA').style.opacity = "1";
                }
                function LSA_stop() {
                  document.getElementById('LSA').style.opacity = "0";
                }
                LSA_stop()
              </script>
            </td>

            <td width="65%" valign="top">
              <a href="https://github.com/mabaorui/TowardsBetterGradient/">
                <papertitle>Towards Better Gradient Consistency for Neural Signed Distance Functions via Level Set Alignment</papertitle>
              </a>
              <br>
              <a href="https://mabaorui.github.io/"> Baorui Ma*</a>, <strong>Junsheng Zhou*</strong>, 
              <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2023
              <br>
              <a href="https://github.com/mabaorui/TowardsBetterGradient/">project page</a> |             
              <a href="http://arxiv.org/abs/2305.11601/">arXiv</a> |
              <a href="https://github.com/mabaorui/TowardsBetterGradient/">code</a>

              <p align="justify", style="font-size:13px">We propose a level set alignment loss to
                evaluate the parallelism of level sets, which can be minimized to achieve better gradient consistency and eliminate uncertainty in the field.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="SMI_stop()" onmouseover="SMI_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="SMI" style="opacity: 0;">
                  <img src='media/smi_before.png' width="225">
                </div>
                <img src="media/smi_after.png" width="225">

              </div>
              <script type="text/javascript">
                function SMI_start() {
                  document.getElementById('SMI').style.opacity = "1";
                }
                function SMI_stop() {
                  document.getElementById('SMI').style.opacity = "0";
                }
                SMI_stop()
              </script>
            </td>

            <td width="65%" valign="top">
              <a href="">
                <papertitle>Multi-Grid Representation with Field Regularization for Self-Supervised Surface Reconstruction from Point Clouds</papertitle>
              </a>
              <br>
              Chuan Jin, <a href="https://sai.jlu.edu.cn/info/1094/3443.htm"> Tieru Wu</a>, <strong>Junsheng Zhou</strong>
              <br>
              <em>Shape Modeling International <b>(SMI)</b></em>, 2023
              &nbsp &nbsp | &nbsp &nbsp 
              <em>Computers & Graphics  <b>(C&G)</b></em>, 2023
              <br>
              <a href="">project page</a> |             
              <a href="">arXiv</a> |
              <a href="">code</a>

              <p align="justify", style="font-size:13px">A research project on 3D vision at Jilin University that I served as the advisor.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tbody>
          <tr onmouseout="neaf_stop()" onmouseover="neaf_start()">
            <td width="28%">
              <div class="one">
                <div class="two" id="neaf" style="opacity: 0;">
                  <img src='media/neaf_before.gif' width="225">
                </div>
                <img src="media/neaf_after.png" width="225">

              </div>
              <script type="text/javascript">
                function neaf_start() {
                  document.getElementById('neaf').style.opacity = "1";
                }
                function neaf_stop() {
                  document.getElementById('neaf').style.opacity = "0";
                }
                neaf_stop()
              </script>
            </td>

            <td width="65%" valign="top">
              <a href="https://github.com/lisj575/NeAF/">
                <papertitle>NeAF: Learning Neural Angle Fields for Point Normal Estimation</papertitle>
              </a>
              <br>
              Shujuan Li*, <strong>Junsheng Zhou*</strong>, <a href="https://mabaorui.github.io/"> Baorui Ma</a>,
              <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
              <br>
              <em>AAAI Conference on Artificial Intelligence <b>(AAAI)</b></em>, 2023 (<strong><span style="color:#ff0000;">Oral</span></strong>)
              <br>
              <a href="https://lisj575.github.io/NeAF/">project page</a> |             
              <a href="https://arxiv.org/pdf/2211.16869.pdf">arXiv</a> |
              <a href="https://github.com/lisj575/NeAF/">code</a>

              <p align="justify", style="font-size:13px">We present NeAF, a novel schema to learn implicit angle fields for point normal estimation. 
                NeAF extends the success of implicit functions (e.g. NeRF and DeepSDF) to normal estimation.
              </p>
            </td>
          </tr>
          </tbody>
        </table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
            <tr onmouseout="capudf_stop()" onmouseover="capudf_start()">
              <td width="28%">
                <div class="one">
                  <div class="two" id="capudf_shape" style="opacity: 0;">
                    <img src='media/capudf_before.png' width="225">
                  </div>
                  <img src="media/capudf_after.png" width="225">

                </div>
                <script type="text/javascript">
                  function capudf_start() {
                    document.getElementById('capudf_shape').style.opacity = "1";
                  }
                  function capudf_stop() {
                    document.getElementById('capudf_shape').style.opacity = "0";
                  }
                  capudf_stop()
                </script>
              </td>
  
              <td width="65%" valign="top">
                <a href="https://junshengzhou.github.io/CAP-UDF/">
                  <papertitle>Learning Consistency-Aware Unsigned Distance Functions Progressively from Raw Point Clouds</papertitle>
                </a>
                <br>
                <strong>Junsheng Zhou*</strong>, <a href="https://mabaorui.github.io/"> Baorui Ma*</a>, <a href="https://yushen-liu.github.io/">Yu-Shen Liu</a>, <a href="http://mmvc.engineering.nyu.edu/">Yi Fang</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
                <br>
                <em>Conference on Neural Information Processing Systems <b>(NeurIPS)</b></em>, 2022 
                <br>
                <a href="https://junshengzhou.github.io/CAP-UDF/">project page</a> |             
                <a href="https://arxiv.org/pdf/2210.02757.pdf">arXiv</a> |
                <a href="https://github.com/junshengzhou/CAP-UDF">code</a>

                <p align="justify", style="font-size:13px">We present CAP-UDF to represent shapes and scenes with arbitrary architecture by learning a Consistency-Aware unsigned distance function Progressively.
                </p>
              </td>
            </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
            <tr onmouseout="attriflow_stop()" onmouseover="attriflow_start()">
              <td width="28%">
                <div class="one">
                  <div class="two" id="attriflow_shape" style="opacity: 0;">
                    <img src='media/attriflow_before.gif' width="225">
                  </div>
                  <img src="media/attriflow_after.png" width="225">

                </div>
                <script type="text/javascript">
                  function attriflow_start() {
                    document.getElementById('attriflow_shape').style.opacity = "1";
                  }
                  function attriflow_stop() {
                    document.getElementById('attriflow_shape').style.opacity = "0";
                  }
                  attriflow_stop()
                </script>
              </td>
  
              <td width="65%" valign="top">
                <a href="https://github.com/junshengzhou/3DAttriFlow">
                  <papertitle>3D Shape Reconstruction from 2D Images with Disentangled Attribute Flow</papertitle>
                </a>
                <br>
                <a href="https://scholar.google.com/citations?user=7gcGzs8AAAAJ&hl=zh-CN&oi=ao"> Xin Wen*</a>, <strong>Junsheng Zhou*</strong>, 
                <a href="https://yushen-liu.github.io/"> Yu-Shen Liu</a>, Hua Su, <a href="https://dongzhenwhu.github.io/index.html">Zhen Dong</a>, <a href="https://h312h.github.io/">Zhizhong Han</a>
                <br>
                <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition <b>(CVPR)</b></em>, 2022 
                <br>
                <a href="https://github.com/junshengzhou/3DAttriFlow">project page</a> |             
                <a href="https://arxiv.org/abs/2203.15190">arXiv</a> |
                <a href="https://github.com/junshengzhou/3DAttriFlow">code</a>

                <p align="justify", style="font-size:13px">We present 3DAttriFlow to obtain high-quality 2D-to-3D reconstruction and point cloud completion results, while allowing controlled semantic attribute editing.
                </p>
              </td>
            </tr>
            </tbody>
          </table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Honors and Awards</heading>
                <ul>

                  <li> <strong>National Scholarship</strong> (å›½å®¶å¥–å­¦é‡‘, Top 1% at Tsinghua University), 2023.
                  </li>
                  <li> <strong>The 3rd Place</strong> in the MVP Completion Challenge (<b>ICCV 2021</b> Workshop)</strong>, 2021.
                  </li>
                  <li> <strong>Best Poster Paper Awards</strong> on ChinaVR, 2021.
                  </li>
                </ul>
    
              </td>
            </tr>
            </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Academic Services</heading>
                <ul>
                  <li>
                    <strong>Co-organizer</strong>: "EMbodied AI: Trends, Challenges, and Opportunities" in ICIP-24 
                  </li>
                  <li> <strong>Program Committee Member</strong>: ICLR-24, IJCAI-24, WWW-24, AAAI-25
                  </li>
                  <li> <strong>Conference Reviewer</strong>: NeurIPS-23, ICML-24, CVPR-23/24, ICCV-23, SIGGRAPH Asia-24, ECCV-24, BMVC-23/24
                  </li>
                  <li> <strong>Journal Reviewer</strong>: CVMJ, ToG
                  </li>

                </ul>
    
              </td>
            </tr>
            </tbody></table>

          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <heading>Academic Services</heading>
              <tr>
                <td>
                  
                  <ul>
                      <li> <strong>Conference Reviewer</strong>: CVPR, ICCV, NeurIPS <br> -->
                      <!-- <li> <strong>Journal Reviewer</strong>: TPAMI, CVIU
                  </ul>
                </td>
              </tr>
          </table> -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tr>
              <td>
              <br>
              <p align="right">
                <font size="2">
                template adapted from <a href="https://jonbarron.info/"><font size="2">this awesome website</font></a>
                <br>
                Last updated: Aug 2024
                <br>
                
              </font>
              </p>
              </td>
            </tr>
          </table>
      </td>
    </tr>
    

</html>
